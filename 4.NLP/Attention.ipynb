{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8c44353-9a19-4007-88fc-f35e81f778e9",
   "metadata": {},
   "source": [
    "# Bahdanau Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83db1769-ba84-4d59-b2ab-1acd6f10b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e2d01-cd95-4962-9201-b37be8c0ab43",
   "metadata": {},
   "source": [
    "## Encdoer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28c7ea18-dce5-4816-8f46-d50edf536bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(tf.keras.layers.Layer):\n",
    "#     def __init__(self, vocab_size, enc_dim=256, num_embedding=256, batch_size=32):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.batch_size = batch_size\n",
    "#         self.enc_dim = enc_dim\n",
    "#         self.num_embedding = num_embedding\n",
    "#         self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.num_embedding)\n",
    "#         self.gru = tf.keras.layers.GRU(enc_dim,\n",
    "#                                       return_sequences=True,\n",
    "#                                       return_state=True,\n",
    "#                                       recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "#     def call(self, x, hidden):\n",
    "#         # 워드 임베딩\n",
    "#         # (batch, seq_length) -> (batch, seq_length, num_embedding)\n",
    "#         x = self.embedding(x)\n",
    "        \n",
    "#         # RNN 출력\n",
    "#         # output.shape: (batch, seq_length, enc_dim)\n",
    "#         # hidden.shape: (batch, enc_dim)\n",
    "#         output, hidden = self.gru(x, initial_state=hidden)\n",
    "        \n",
    "#         return output, hidden\n",
    "    \n",
    "#     def init_hidden(self, input_):\n",
    "#         return tf.zeros((tf.shape(input_)[0], self.enc_dim))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "334414c4-c5c6-4c53-8d07-a0b397b6d421",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, enc_dim=256, num_embedding=256, batch_size=32):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_dim = enc_dim\n",
    "        self.num_embedding = num_embedding\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.num_embedding)\n",
    "        \n",
    "        self.gru_fw = tf.keras.layers.GRU(enc_dim,\n",
    "                                          return_sequences=True,\n",
    "                                          return_state=True,\n",
    "                                          recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        self.gru_bw = tf.keras.layers.GRU(enc_dim,\n",
    "                                          go_backwards=True,\n",
    "                                          return_sequences=True,\n",
    "                                          return_state=True,\n",
    "                                          recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        self.gru = tf.keras.layers.Bidirectional(self.gru_fw, backward_layer=self.gru_bw)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        # 워드 임베딩\n",
    "        # (batch, seq_length) -> (batch, seq_length, num_embedding)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # RNN 출력\n",
    "        # output.shape: (batch, seq_length, enc_dim * 2)\n",
    "        # fw_hidden.shape: (batch, enc_dim)\n",
    "        # bw_hidden.shape: (batch, enc_dim)\n",
    "        output, fw_hidden, bw_hidden = self.gru(x, initial_state=hidden)\n",
    "        \n",
    "        hidden = tf.concat([fw_hidden, bw_hidden], axis=-1)  # (bs, d_model * 2)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, input_):\n",
    "        return [tf.zeros((tf.shape(input_)[0], self.enc_dim)) for _ in range(2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0ed349-2382-42d7-9b53-98091d1618ef",
   "metadata": {},
   "source": [
    "## Bahdanau Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "296c6c32-edc5-487e-be8e-7f6d71d2488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.models.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "    \n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, encoder_out, hidden):\n",
    "        # output.shape: (batch, seq_length, enc_dim)\n",
    "        # hidden.shape: (batch, enc_dim)\n",
    "        \n",
    "        # hidden에 시계열 축 추가\n",
    "        hidden = tf.expand_dims(hidden, axis=1) #out: (16, 1, 1024)\n",
    "        \n",
    "        # Bahdanau attention score 계산\n",
    "        # (batch, enc_dim) -> (batch, 1, enc_dim)\n",
    "        score = self.V(tf.nn.tanh(self.W1(encoder_out) +\\\n",
    "                                  self.W2(hidden))) #out: \n",
    "        \n",
    "        # softmax를 통해 attention weights 계산\n",
    "        attn_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context vector 계산\n",
    "        # ((batch, 1, enc_dim) * (batch, seq_length, enc_dim)) -> (batch, seq_length, enc_dim)\n",
    "        context_vector =  attn_weights * encoder_out\n",
    "        \n",
    "        # (batch, seq_length, enc_dim) -> (batch, enc_dim)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1) \n",
    "        return context_vector, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9c48b-db52-4f9b-8b89-409299c7a589",
   "metadata": {},
   "source": [
    "## Decoder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49163ae5-70be-42fd-b828-c9fa55fb905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.models.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim=256, dec_dim=256, batch_size=32):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dec_dim = dec_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.attn = BahdanauAttention(self.dec_dim)\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_dim,\n",
    "                                    recurrent_initializer='glorot_uniform',\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(self.vocab_size)\n",
    "        \n",
    "    def call(self, x, hidden, enc_out):\n",
    "        # x.shape = (None, 1)\n",
    "        # enc_out.shape = (None, seq_length, enc_dim)\n",
    "        # enc_hidden.shape = (None, enc_dim)\n",
    "        \n",
    "        # decoder input의 워드 임베딩\n",
    "        # (None, 1) -> (None, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # attention 가중치 계산\n",
    "        # context.shape = (None, enc_dim)\n",
    "        # attn_weights.shape = (None, seq_Length, enc_dim)\n",
    "        context, attn_weights = self.attn(enc_out, hidden)\n",
    "        \n",
    "        # x.shape = (None, 1, enc_dim + embedding_dim)\n",
    "        x = tf.concat((tf.expand_dims(context, 1), x), -1)\n",
    "        \n",
    "        # Decoder RNN sequence 출력\n",
    "        # r_out.shape = (None, 1, dec_dim)\n",
    "        # r_out.shape = (None, dec_dim)\n",
    "        r_out, hidden = self.gru(x, initial_state=hidden)\n",
    "        \n",
    "        # 시계열 축 제거\n",
    "        # (None, 1, dec_dim) -> (None, dec_dim)\n",
    "        out = tf.reshape(r_out,shape=(-1, r_out.shape[2]))\n",
    "        \n",
    "        \n",
    "        return self.fc(out), hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125bc922-5681-4d5b-8dfb-5d680cae3568",
   "metadata": {},
   "source": [
    "## Loss & Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95446599-7dcf-4ddc-8418-8c55ef75e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                            reduction='none')\n",
    "\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
    "\n",
    "def loss(real, pred):\n",
    "    # [PAD] - 0 태그를 빼고 loss를 구하기 위해\n",
    "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def accuracy(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
    "    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\n",
    "    pred *= mask\n",
    "    acc = train_accuracy(real, pred)\n",
    "    \n",
    "    return tf.reduce_mean(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fa3774-28d1-41a4-9635-1f1fe344b72c",
   "metadata": {},
   "source": [
    "## Main Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f4c9465-f8d7-4481-9cbc-a0aceb506035",
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq(tf.keras.Model):\n",
    "    def __init__(self, enc_vocab_size, dec_vocab_size, embedding_dim, enc_dim, dec_dim, batch_size, end_token_idx=3):\n",
    "        super(seq2seq, self).__init__()\n",
    "        # 문장의 끝 토큰 [EOS] index - 3 \n",
    "        self.end_token_idx = end_token_idx\n",
    "        \n",
    "        self.encoder = Encoder(enc_vocab_size, embedding_dim, enc_dim, batch_size)\n",
    "        self.decoder = Decoder(dec_vocab_size, embedding_dim, dec_dim, batch_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        # encoder, decoder input\n",
    "        input_, target = x\n",
    "        \n",
    "        # encoder 초기값 설정\n",
    "        enc_hidden = self.encoder.init_hidden(input_)\n",
    "        # encoder의 RNN 연산 후 출력값\n",
    "        enc_out, enc_hidden = self.encoder(input_, enc_hidden)\n",
    "        \n",
    "        # dec_hidden 초기값 지정\n",
    "        dec_hidden = enc_hidden\n",
    "        \n",
    "        predict_tokens = list()\n",
    "        for t in range(target.shape[1]):\n",
    "            # decoder input에 시계열 축 추가 (None, 1, 1)\n",
    "            dec_input = tf.dtypes.cast(tf.expand_dims(target[:, t], 1), tf.float32)\n",
    "            # decoder RNN 연산 결과\n",
    "            predictions, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_out)\n",
    "            predict_tokens.append(tf.dtypes.cast(predictions, tf.float32))\n",
    "            \n",
    "        return tf.stack(predict_tokens, axis=1)\n",
    "    \n",
    "    def inference(self, x):\n",
    "        input_ = x\n",
    "        \n",
    "        enc_hidden = self.encoder.init_hidden(input_)\n",
    "        enc_out, enc_hidden = self.encoder(input_, enc_hidden)\n",
    "        \n",
    "        dec_hidden = enc_hidden\n",
    "        \n",
    "        dec_input = np.array([2]) # [BOF] index\n",
    "        dec_input = tf.expand_dims(dec_input, 1)\n",
    "        \n",
    "        predict_tokens = list()\n",
    "        for t in range(0, MAX_SEQUENCE):\n",
    "            predictions, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_out)\n",
    "            predict_token = tf.argmax(predictions[0])\n",
    "            \n",
    "            if predict_token == self.end_token_idx:\n",
    "                break\n",
    "                \n",
    "            predict_tokens.append(predict_token)\n",
    "            dec_input = tf.dtypes.cast(tf.expand_dims([predict_token], 0), tf.float32)\n",
    "        \n",
    "        return tf.stack(predict_tokens, axis=0).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a704ad2-33a3-429a-a4aa-9ded323716ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5408bcf3-f18c-4b98-ba9c-d3d6941ab42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190200</th>\n",
       "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
       "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190201</th>\n",
       "      <td>Death is something that we're often discourage...</td>\n",
       "      <td>La mort est une chose qu'on nous décourage sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190202</th>\n",
       "      <td>Since there are usually multiple websites on a...</td>\n",
       "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190203</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190204</th>\n",
       "      <td>It may be impossible to get a completely error...</td>\n",
       "      <td>Il est peut-être impossible d'obtenir un Corpu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190205 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      eng  \\\n",
       "0                                                     Go.   \n",
       "1                                                     Go.   \n",
       "2                                                     Hi.   \n",
       "3                                                     Hi.   \n",
       "4                                                    Run!   \n",
       "...                                                   ...   \n",
       "190200  A carbon footprint is the amount of carbon dio...   \n",
       "190201  Death is something that we're often discourage...   \n",
       "190202  Since there are usually multiple websites on a...   \n",
       "190203  If someone who doesn't know your background sa...   \n",
       "190204  It may be impossible to get a completely error...   \n",
       "\n",
       "                                                      fra  \n",
       "0                                                 Marche.  \n",
       "1                                                 Bouge !  \n",
       "2                                                 Salut !  \n",
       "3                                                  Salut.  \n",
       "4                                                 Cours !  \n",
       "...                                                   ...  \n",
       "190200  Une empreinte carbone est la somme de pollutio...  \n",
       "190201  La mort est une chose qu'on nous décourage sou...  \n",
       "190202  Puisqu'il y a de multiples sites web sur chaqu...  \n",
       "190203  Si quelqu'un qui ne connaît pas vos antécédent...  \n",
       "190204  Il est peut-être impossible d'obtenir un Corpu...  \n",
       "\n",
       "[190205 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/fra.txt', sep='\\t')\n",
    "data.columns=['eng', 'fra', 'etc']\n",
    "data.drop('etc', axis=1, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5507dbac-3614-4624-a70f-7a288408ccdb",
   "metadata": {},
   "source": [
    "## 전처리\n",
    "* 띄어쓰기 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba7a6a9b-2c0c-434a-89f8-1bf53ca7cf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(x):\n",
    "    text_eng = x['eng']\n",
    "    text_fra = x['fra']\n",
    "    \n",
    "    text_eng = re.sub(r\"([!.,?])\", r\" \\1\", text_eng)\n",
    "    text_fra = re.sub(r\"([!.,?])\", r\" \\1\", text_fra)\n",
    "    \n",
    "    text_eng = re.sub(r\"[^a-zA-Z?.,!]+\", \" \", text_eng)\n",
    "    text_fra = re.sub(r\"[^a-zA-Z?.,!]+\", \" \", text_fra)\n",
    "    \n",
    "    return text_eng, text_fra\n",
    "\n",
    "def sentence2length(x):\n",
    "    x['preprocessed_eng'].split()\n",
    "    x['preprocessed_fra'].split()\n",
    "    \n",
    "    return eng, fra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37822cf3-a982-414a-82c6-5a2611a38f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>preprocessed_eng</th>\n",
       "      <th>preprocessed_fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>190200</th>\n",
       "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
       "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
       "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
       "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190201</th>\n",
       "      <td>Death is something that we're often discourage...</td>\n",
       "      <td>La mort est une chose qu'on nous décourage sou...</td>\n",
       "      <td>Death is something that we re often discourage...</td>\n",
       "      <td>La mort est une chose qu on nous d courage sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190202</th>\n",
       "      <td>Since there are usually multiple websites on a...</td>\n",
       "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
       "      <td>Since there are usually multiple websites on a...</td>\n",
       "      <td>Puisqu il y a de multiples sites web sur chaqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190203</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n",
       "      <td>If someone who doesn t know your background sa...</td>\n",
       "      <td>Si quelqu un qui ne conna t pas vos ant c dent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190204</th>\n",
       "      <td>It may be impossible to get a completely error...</td>\n",
       "      <td>Il est peut-être impossible d'obtenir un Corpu...</td>\n",
       "      <td>It may be impossible to get a completely error...</td>\n",
       "      <td>Il est peut tre impossible d obtenir un Corpus...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      eng  \\\n",
       "190200  A carbon footprint is the amount of carbon dio...   \n",
       "190201  Death is something that we're often discourage...   \n",
       "190202  Since there are usually multiple websites on a...   \n",
       "190203  If someone who doesn't know your background sa...   \n",
       "190204  It may be impossible to get a completely error...   \n",
       "\n",
       "                                                      fra  \\\n",
       "190200  Une empreinte carbone est la somme de pollutio...   \n",
       "190201  La mort est une chose qu'on nous décourage sou...   \n",
       "190202  Puisqu'il y a de multiples sites web sur chaqu...   \n",
       "190203  Si quelqu'un qui ne connaît pas vos antécédent...   \n",
       "190204  Il est peut-être impossible d'obtenir un Corpu...   \n",
       "\n",
       "                                         preprocessed_eng  \\\n",
       "190200  A carbon footprint is the amount of carbon dio...   \n",
       "190201  Death is something that we re often discourage...   \n",
       "190202  Since there are usually multiple websites on a...   \n",
       "190203  If someone who doesn t know your background sa...   \n",
       "190204  It may be impossible to get a completely error...   \n",
       "\n",
       "                                         preprocessed_fra  \n",
       "190200  Une empreinte carbone est la somme de pollutio...  \n",
       "190201  La mort est une chose qu on nous d courage sou...  \n",
       "190202  Puisqu il y a de multiples sites web sur chaqu...  \n",
       "190203  Si quelqu un qui ne conna t pas vos ant c dent...  \n",
       "190204  Il est peut tre impossible d obtenir un Corpus...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['preprocessed_eng'] = data.apply(lambda x: preprocessing(x)[0], axis=1)\n",
    "data['preprocessed_fra'] = data.apply(lambda x: preprocessing(x)[1], axis=1)\n",
    "data.tail()\n",
    "# seq_length = data.apply(lambda x: data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "391a998a-46fb-4db5-8b1c-46211e56a21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 66\n"
     ]
    }
   ],
   "source": [
    "# 가장 긴 문장은?\n",
    "engLen = data['preprocessed_eng'].apply(lambda x: len(x.split()))\n",
    "fraLen = data['preprocessed_fra'].apply(lambda x: len(x.split()))\n",
    "print(engLen.max(), fraLen.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b64be4bd-3e19-477d-85f6-342c5217cf30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>preprocessed_eng</th>\n",
       "      <th>preprocessed_fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "      <td>Go .</td>\n",
       "      <td>Marche .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "      <td>Go .</td>\n",
       "      <td>Bouge !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "      <td>Hi .</td>\n",
       "      <td>Salut !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut.</td>\n",
       "      <td>Hi .</td>\n",
       "      <td>Salut .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "      <td>Run !</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190198</th>\n",
       "      <td>We need to uphold laws against discrimination ...</td>\n",
       "      <td>Nous devons faire respecter les lois contre la...</td>\n",
       "      <td>We need to uphold laws against discrimination ...</td>\n",
       "      <td>Nous devons faire respecter les lois contre la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190199</th>\n",
       "      <td>Top-down economics never works, said Obama. \"T...</td>\n",
       "      <td>« L'économie en partant du haut vers le bas, ç...</td>\n",
       "      <td>Top down economics never works , said Obama . ...</td>\n",
       "      <td>L conomie en partant du haut vers le bas , a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190200</th>\n",
       "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
       "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
       "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
       "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190201</th>\n",
       "      <td>Death is something that we're often discourage...</td>\n",
       "      <td>La mort est une chose qu'on nous décourage sou...</td>\n",
       "      <td>Death is something that we re often discourage...</td>\n",
       "      <td>La mort est une chose qu on nous d courage sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190202</th>\n",
       "      <td>Since there are usually multiple websites on a...</td>\n",
       "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
       "      <td>Since there are usually multiple websites on a...</td>\n",
       "      <td>Puisqu il y a de multiples sites web sur chaqu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190203 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      eng  \\\n",
       "0                                                     Go.   \n",
       "1                                                     Go.   \n",
       "2                                                     Hi.   \n",
       "3                                                     Hi.   \n",
       "4                                                    Run!   \n",
       "...                                                   ...   \n",
       "190198  We need to uphold laws against discrimination ...   \n",
       "190199  Top-down economics never works, said Obama. \"T...   \n",
       "190200  A carbon footprint is the amount of carbon dio...   \n",
       "190201  Death is something that we're often discourage...   \n",
       "190202  Since there are usually multiple websites on a...   \n",
       "\n",
       "                                                      fra  \\\n",
       "0                                                 Marche.   \n",
       "1                                                 Bouge !   \n",
       "2                                                 Salut !   \n",
       "3                                                  Salut.   \n",
       "4                                                 Cours !   \n",
       "...                                                   ...   \n",
       "190198  Nous devons faire respecter les lois contre la...   \n",
       "190199  « L'économie en partant du haut vers le bas, ç...   \n",
       "190200  Une empreinte carbone est la somme de pollutio...   \n",
       "190201  La mort est une chose qu'on nous décourage sou...   \n",
       "190202  Puisqu'il y a de multiples sites web sur chaqu...   \n",
       "\n",
       "                                         preprocessed_eng  \\\n",
       "0                                                    Go .   \n",
       "1                                                    Go .   \n",
       "2                                                    Hi .   \n",
       "3                                                    Hi .   \n",
       "4                                                   Run !   \n",
       "...                                                   ...   \n",
       "190198  We need to uphold laws against discrimination ...   \n",
       "190199  Top down economics never works , said Obama . ...   \n",
       "190200  A carbon footprint is the amount of carbon dio...   \n",
       "190201  Death is something that we re often discourage...   \n",
       "190202  Since there are usually multiple websites on a...   \n",
       "\n",
       "                                         preprocessed_fra  \n",
       "0                                                Marche .  \n",
       "1                                                 Bouge !  \n",
       "2                                                 Salut !  \n",
       "3                                                 Salut .  \n",
       "4                                                 Cours !  \n",
       "...                                                   ...  \n",
       "190198  Nous devons faire respecter les lois contre la...  \n",
       "190199   L conomie en partant du haut vers le bas , a ...  \n",
       "190200  Une empreinte carbone est la somme de pollutio...  \n",
       "190201  La mort est une chose qu on nous d courage sou...  \n",
       "190202  Puisqu il y a de multiples sites web sur chaqu...  \n",
       "\n",
       "[190203 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = data[data.preprocessed_eng.str.split().str.len() < 50]\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d8071-5548-4147-8b9f-3b5b34211d29",
   "metadata": {},
   "source": [
    "## 영어, 프랑스어 단어 사전 만들기(word->id, id->word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fd1069a-4d0c-4ece-b781-992d10ef4d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_word_to_id = {'[PAD]': 0, '[UNK]': 1, '[BOS]': 2, '[EOS]': 3}\n",
    "fra_word_to_id = {'[PAD]': 0, '[UNK]': 1, '[BOS]': 2, '[EOS]': 3}\n",
    "\n",
    "words_eng = \"\"\n",
    "for eng in data_df['preprocessed_eng']:\n",
    "    words_eng += eng + \" \"\n",
    "words_eng = list(set(words_eng.split()))\n",
    "\n",
    "for word_eng in words_eng:\n",
    "    eng_word_to_id[word_eng] = len(eng_word_to_id)\n",
    "\n",
    "words_fra = \"\"\n",
    "for fra in data_df['preprocessed_fra']:\n",
    "    words_fra += fra + \" \"\n",
    "words_fra = list(set(words_fra.split()))\n",
    "\n",
    "for word_fra in words_fra:\n",
    "    fra_word_to_id[word_fra] = len(fra_word_to_id)\n",
    "\n",
    "# print(len(eng_word_to_id), len(fra_word_to_id)) : (16687, 22665)\n",
    "\n",
    "# 각 숫자별 단어 부여\n",
    "id_to_eng_word = {_id:word for word, _id in eng_word_to_id.items()}\n",
    "id_to_fra_word = {_id:word for word, _id in fra_word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d958155d-bafc-431f-9a42-67cdd2e3f96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((190203, 50), (190203, 70), (190203, 70))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = []\n",
    "labels = []\n",
    "ENG_MAX_SEQUENCE = 50\n",
    "FRA_MAX_SEQUENCE = 70\n",
    "\n",
    "shffled_data = data_df.sample(frac=1).copy()\n",
    "shffled_data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "inputs_eng = []\n",
    "\n",
    "for eng_sentence in shffled_data['preprocessed_eng']:\n",
    "        row = [eng_word_to_id[eng_word] for eng_word in eng_sentence.split()]\n",
    "        row += [0] * (ENG_MAX_SEQUENCE - len(row))\n",
    "        inputs_eng.append(row)\n",
    "\n",
    "inputs_fra = []\n",
    "for fra_sentence in shffled_data['preprocessed_fra']:\n",
    "    # decoder 입력과 label 생성\n",
    "    row = [fra_word_to_id['[BOS]']]\n",
    "    row_label = [fra_word_to_id[fra_word] for fra_word in fra_sentence.split()]\n",
    "    row += row_label\n",
    "    row_label += [fra_word_to_id['[EOS]']]\n",
    "    row += [0] * (FRA_MAX_SEQUENCE - len(row))\n",
    "    row_label += [0] * (FRA_MAX_SEQUENCE - len(row_label))\n",
    "    inputs_fra.append(row)\n",
    "    labels.append(row_label)\n",
    "    \n",
    "enc_inputs = np.array(inputs_eng)\n",
    "dec_inputs = np.array(inputs_fra)\n",
    "labels = np.array(labels)\n",
    "\n",
    "enc_inputs.shape, dec_inputs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41047929-ebcf-44a7-8f64-93fcb203e2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape:  (114121, 50) (114121, 70) (114121, 70)\n",
      "validation data shape:  (38041, 50) (38041, 70) (38041, 70)\n",
      "test data shape:  (38041, 50) (38041, 70) (38041, 70)\n"
     ]
    }
   ],
   "source": [
    "train_idx = int(len(labels) * 0.6)\n",
    "val_idx = (len(labels) - train_idx)//2 + train_idx\n",
    "\n",
    "train_enc_inputs = enc_inputs[:train_idx,:]\n",
    "train_dec_inputs = dec_inputs[:train_idx]\n",
    "val_enc_inputs = enc_inputs[train_idx:val_idx,:]\n",
    "val_dec_inputs = dec_inputs[train_idx:val_idx]\n",
    "test_enc_inputs = enc_inputs[val_idx:,:]\n",
    "test_dec_inputs = dec_inputs[val_idx:]\n",
    "\n",
    "# train_X = inputs[:,:train_idx]\n",
    "# val_X = inputs[:,train_idx:val_idx]\n",
    "# test_X = inputs[:,val_idx:]\n",
    "\n",
    "train_y = labels[:train_idx]\n",
    "val_y = labels[train_idx:val_idx]\n",
    "test_y = labels[val_idx:]\n",
    "\n",
    "# train_X.shape, val_X.shape, test_X.shape\n",
    "print(\"train data shape: \",train_enc_inputs.shape, train_dec_inputs.shape, train_y.shape)\n",
    "print(\"validation data shape: \",val_enc_inputs.shape, val_dec_inputs.shape, val_y.shape)\n",
    "print(\"test data shape: \",test_enc_inputs.shape, test_dec_inputs.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dfb1d8-daa7-412d-bdd2-ee58ebe2d02e",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c07f34a9-aabd-4ee7-9ad4-6204b9dfdcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dim = 256\n",
    "dec_dim = 512 # 512 # bidirection\n",
    "embedding_dim = 256\n",
    "batch_size = 32\n",
    "EPOCH = 50\n",
    "\n",
    "model = seq2seq(len(eng_word_to_id),\n",
    "                len(fra_word_to_id),\n",
    "                embedding_dim,\n",
    "                enc_dim,\n",
    "                dec_dim,\n",
    "                batch_size)\n",
    "\n",
    "model.compile(loss=loss,\n",
    "             optimizer=tf.keras.optimizers.Adam(),\n",
    "             metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c217735-56b6-4e1a-84c1-b7150781c601",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3567/3567 [==============================] - 1324s 355ms/step - loss: 0.5973 - accuracy: 0.9777 - val_loss: 0.2786 - val_accuracy: 0.9771\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.97710, saving model to model/weights-50+01.h5\n",
      "Epoch 2/50\n",
      "3567/3567 [==============================] - 1254s 351ms/step - loss: 0.2277 - accuracy: 0.9769 - val_loss: 0.2136 - val_accuracy: 0.9767\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.97710\n",
      "Epoch 3/50\n",
      "3567/3567 [==============================] - 1253s 351ms/step - loss: 0.1480 - accuracy: 0.9766 - val_loss: 0.1964 - val_accuracy: 0.9765\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.97710\n",
      "Epoch 4/50\n",
      "3567/3567 [==============================] - 1253s 351ms/step - loss: 0.1103 - accuracy: 0.9764 - val_loss: 0.1903 - val_accuracy: 0.9764\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.97710\n",
      "Epoch 5/50\n",
      "3567/3567 [==============================] - 1253s 351ms/step - loss: 0.0895 - accuracy: 0.9763 - val_loss: 0.1915 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.97710\n",
      "Epoch 6/50\n",
      "3567/3567 [==============================] - 1252s 351ms/step - loss: 0.0758 - accuracy: 0.9762 - val_loss: 0.1923 - val_accuracy: 0.9762\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.97710\n",
      "Epoch 7/50\n",
      "3567/3567 [==============================] - 1253s 351ms/step - loss: 0.0655 - accuracy: 0.9761 - val_loss: 0.1955 - val_accuracy: 0.9761\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.97710\n",
      "Epoch 8/50\n",
      "3567/3567 [==============================] - 1254s 351ms/step - loss: 0.0585 - accuracy: 0.9761 - val_loss: 0.2013 - val_accuracy: 0.9761\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.97710\n",
      "Epoch 9/50\n",
      "3567/3567 [==============================] - 1253s 351ms/step - loss: 0.0538 - accuracy: 0.9761 - val_loss: 0.2050 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.97710\n",
      "Epoch 10/50\n",
      "3567/3567 [==============================] - 1254s 351ms/step - loss: 0.0492 - accuracy: 0.9761 - val_loss: 0.2104 - val_accuracy: 0.9761\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.97710\n",
      "Epoch 11/50\n",
      "3567/3567 [==============================] - 1253s 351ms/step - loss: 0.0461 - accuracy: 0.9761 - val_loss: 0.2139 - val_accuracy: 0.9761\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.97710\n",
      "Epoch 12/50\n",
      "3567/3567 [==============================] - 1252s 351ms/step - loss: 0.0441 - accuracy: 0.9762 - val_loss: 0.2194 - val_accuracy: 0.9762\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.97710\n",
      "Epoch 13/50\n",
      "3567/3567 [==============================] - 1252s 351ms/step - loss: 0.0417 - accuracy: 0.9762 - val_loss: 0.2221 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.97710\n",
      "Epoch 14/50\n",
      "3567/3567 [==============================] - 1253s 351ms/step - loss: 0.0400 - accuracy: 0.9763 - val_loss: 0.2267 - val_accuracy: 0.9764\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.97710\n",
      "Epoch 15/50\n",
      "3567/3567 [==============================] - 1254s 352ms/step - loss: 0.0388 - accuracy: 0.9765 - val_loss: 0.2299 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.97710\n",
      "Epoch 16/50\n",
      "3567/3567 [==============================] - 1253s 351ms/step - loss: 0.0378 - accuracy: 0.9766 - val_loss: 0.2344 - val_accuracy: 0.9767\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.97710\n",
      "Epoch 17/50\n",
      "3567/3567 [==============================] - 1254s 352ms/step - loss: 0.0367 - accuracy: 0.9767 - val_loss: 0.2373 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.97710\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "MODEL_PATH = \"model/\"\n",
    "if not(os.path.isdir(MODEL_PATH)):\n",
    "    os.makedirs(os.path.join(MODEL_PATH))\n",
    "    \n",
    "checkpoint_path = MODEL_PATH + 'weights-50+{epoch:02d}.h5'\n",
    "\n",
    "cp_callback = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', verbose=1,\n",
    "                              save_best_only=True, save_weights_only=True)\n",
    "\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=16)\n",
    "\n",
    "history = model.fit([train_enc_inputs, train_dec_inputs], train_y,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=EPOCH,\n",
    "                    validation_data=((val_enc_inputs, val_dec_inputs), val_y),\n",
    "                    callbacks=[earlystop_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac035067-f7f7-4124-9615-c3f877c1fe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "print('end')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8220c0d5-b895-4ff3-a60f-6f9611a57463",
   "metadata": {},
   "source": [
    "# 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed4778df-f95c-4b30-a5fc-fe23edd136ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,) (70,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1, 50), (1, 70))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs (encoder, decoder)과 같은 shape의 array 생성\n",
    "enc_input_shape, dec_input_shape = np.zeros(ENG_MAX_SEQUENCE), np.zeros(FRA_MAX_SEQUENCE)\n",
    "print(enc_input_shape.shape, dec_input_shape.shape)\n",
    "\n",
    "enc_input_shape = enc_input_shape.reshape(-1,enc_input_shape.shape[0])\n",
    "dec_input_shape = dec_input_shape.reshape(-1,dec_input_shape.shape[0])\n",
    "\n",
    "enc_input_shape.shape, dec_input_shape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77de1194-7c62-4e1e-956a-8cd8bb06e94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"seq2seq_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_2 (Encoder)          multiple                  5061376   \n",
      "_________________________________________________________________\n",
      "decoder_2 (Decoder)          multiple                  19924362  \n",
      "=================================================================\n",
      "Total params: 24,985,738\n",
      "Trainable params: 24,985,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 가중치를 가져오기 위한 모델 선언\n",
    "model([enc_input_shape, dec_input_shape])\n",
    "\n",
    "# 가중치 불러오기\n",
    "model.load_weights('model/weights-50+01.h5')\n",
    "\n",
    "# 모델 구조 확ㅇ인\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "270e2d1b-1c92-46cd-8893-637f48e2e445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 문장\n",
      "I m not very musically inclined .\n",
      "번역 문장(Attention model)\n",
      "Je ne suis pas tr s flatt e .\n",
      "올바른 문장(Correct sentence)\n",
      "Je n ai pas l me tr s musicale .\n"
     ]
    }
   ],
   "source": [
    "sentence_num = 1654\n",
    "# 1문장 확인하기\n",
    "sentence = test_enc_inputs[sentence_num]\n",
    "# print(sentence)\n",
    "\n",
    "# 영어 문장 출력\n",
    "print(\"입력 문장\")\n",
    "print(' '.join([id_to_eng_word[token] for token in sentence if token != 0]))\n",
    "\n",
    "# 모델로 번역하기\n",
    "sentence = sentence.reshape(-1, sentence.shape[0])\n",
    "pred = model.inference(sentence)\n",
    "\n",
    "# 번역된 문장 출력\n",
    "print(\"번역 문장(Attention model)\")\n",
    "print(' '.join([id_to_fra_word[token] for token in pred if token != 0]))\n",
    "\n",
    "# 기존 문장 출력\n",
    "print(\"올바른 문장(Correct sentence)\")\n",
    "print(' '.join([id_to_fra_word[token] for token in test_dec_inputs[sentence_num] if token != 0 and token != 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7d2dbf-a1a4-489d-8166-f45bc2460731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
