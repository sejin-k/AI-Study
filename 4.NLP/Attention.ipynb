{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8c44353-9a19-4007-88fc-f35e81f778e9",
   "metadata": {},
   "source": [
    "# Bahdanau Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83db1769-ba84-4d59-b2ab-1acd6f10b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e2d01-cd95-4962-9201-b37be8c0ab43",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Encdoer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28c7ea18-dce5-4816-8f46-d50edf536bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, enc_dim=256, num_embedding=256, batch_size=32):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_dim = enc_dim\n",
    "        self.num_embedding = num_embedding\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.num_embedding)\n",
    "        self.gru = tf.keras.layers.GRU(enc_dim,\n",
    "                                      return_sequences=True,\n",
    "                                      return_state=True,\n",
    "                                      recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        # 워드 임베딩\n",
    "        # (batch, seq_length) -> (batch, seq_length, num_embedding)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # RNN 출력\n",
    "        # output.shape: (batch, seq_length, enc_dim)\n",
    "        # hidden.shape: (batch, enc_dim)\n",
    "        output, hidden = self.gru(x, initial_state=hidden)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, input_):\n",
    "        return tf.zeros((tf.shape(input_)[0], self.enc_dim))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c2a696-10c7-44b8-9c36-41b7f305fa2d",
   "metadata": {},
   "source": [
    "### Bidirectional RNN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "334414c4-c5c6-4c53-8d07-a0b397b6d421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(tf.keras.layers.Layer):\n",
    "#     def __init__(self, vocab_size, enc_dim=256, num_embedding=256, batch_size=32):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.batch_size = batch_size\n",
    "#         self.enc_dim = enc_dim\n",
    "#         self.num_embedding = num_embedding\n",
    "#         self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.num_embedding)\n",
    "        \n",
    "#         self.gru_fw = tf.keras.layers.GRU(enc_dim,\n",
    "#                                           return_sequences=True,\n",
    "#                                           return_state=True,\n",
    "#                                           recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "#         self.gru_bw = tf.keras.layers.GRU(enc_dim,\n",
    "#                                           go_backwards=True,\n",
    "#                                           return_sequences=True,\n",
    "#                                           return_state=True,\n",
    "#                                           recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "#         self.gru = tf.keras.layers.Bidirectional(self.gru_fw, backward_layer=self.gru_bw)\n",
    "        \n",
    "#     def call(self, x, hidden):\n",
    "#         # 워드 임베딩\n",
    "#         # (batch, seq_length) -> (batch, seq_length, num_embedding)\n",
    "#         x = self.embedding(x)\n",
    "        \n",
    "#         # RNN 출력\n",
    "#         # output.shape: (batch, seq_length, enc_dim * 2)\n",
    "#         # fw_hidden.shape: (batch, enc_dim)\n",
    "#         # bw_hidden.shape: (batch, enc_dim)\n",
    "#         output, fw_hidden, bw_hidden = self.gru(x, initial_state=hidden)\n",
    "        \n",
    "#         hidden = tf.concat([fw_hidden, bw_hidden], axis=-1)  # (bs, d_model * 2)\n",
    "        \n",
    "#         return output, hidden\n",
    "    \n",
    "#     def init_hidden(self, input_):\n",
    "#         return [tf.zeros((tf.shape(input_)[0], self.enc_dim)) for _ in range(2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0ed349-2382-42d7-9b53-98091d1618ef",
   "metadata": {},
   "source": [
    "## Bahdanau Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "296c6c32-edc5-487e-be8e-7f6d71d2488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.models.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "    \n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, encoder_out, hidden):\n",
    "        # output.shape: (batch, seq_length, enc_dim)\n",
    "        # hidden.shape: (batch, enc_dim)\n",
    "        \n",
    "        # hidden에 시계열 축 추가\n",
    "        hidden = tf.expand_dims(hidden, axis=1) #out: (16, 1, 1024)\n",
    "        \n",
    "        # Bahdanau attention score 계산\n",
    "        # (batch, enc_dim) -> (batch, 1, enc_dim)\n",
    "        score = self.V(tf.nn.tanh(self.W1(encoder_out) +\\\n",
    "                                  self.W2(hidden))) #out: \n",
    "        \n",
    "        # softmax를 통해 attention weights 계산\n",
    "        attn_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context vector 계산\n",
    "        # ((batch, 1, enc_dim) * (batch, seq_length, enc_dim)) -> (batch, seq_length, enc_dim)\n",
    "        context_vector =  attn_weights * encoder_out\n",
    "        \n",
    "        # (batch, seq_length, enc_dim) -> (batch, enc_dim)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1) \n",
    "        return context_vector, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9c48b-db52-4f9b-8b89-409299c7a589",
   "metadata": {},
   "source": [
    "## Decoder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49163ae5-70be-42fd-b828-c9fa55fb905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.models.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim=256, dec_dim=256, batch_size=32):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dec_dim = dec_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.attn = BahdanauAttention(self.dec_dim)\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_dim,\n",
    "                                       recurrent_initializer='glorot_uniform',\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(self.vocab_size)\n",
    "        \n",
    "    def call(self, x, hidden, enc_out):\n",
    "        # x.shape = (None, 1)\n",
    "        # enc_out.shape = (None, seq_length, enc_dim)\n",
    "        # enc_hidden.shape = (None, enc_dim)\n",
    "        \n",
    "        # decoder input의 워드 임베딩\n",
    "        # (None, 1) -> (None, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # attention 가중치 계산\n",
    "        # context.shape = (None, enc_dim)\n",
    "        # attn_weights.shape = (None, seq_Length, enc_dim)\n",
    "        context, attn_weights = self.attn(enc_out, hidden)\n",
    "        \n",
    "        # x.shape = (None, 1, enc_dim + embedding_dim)\n",
    "        x = tf.concat((tf.expand_dims(context, 1), x), -1)\n",
    "        \n",
    "        # Decoder RNN sequence 출력\n",
    "        # r_out.shape = (None, 1, dec_dim)\n",
    "        # r_out.shape = (None, dec_dim)\n",
    "        r_out, hidden = self.gru(x, initial_state=hidden)\n",
    "        \n",
    "        # 시계열 축 제거\n",
    "        # (None, 1, dec_dim) -> (None, dec_dim)\n",
    "        out = tf.reshape(r_out,shape=(-1, r_out.shape[2]))\n",
    "        \n",
    "        \n",
    "        return self.fc(out), hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125bc922-5681-4d5b-8dfb-5d680cae3568",
   "metadata": {},
   "source": [
    "## Loss & Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95446599-7dcf-4ddc-8418-8c55ef75e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                            reduction='none')\n",
    "\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
    "\n",
    "def loss(real, pred):\n",
    "    # [PAD] - 0 태그를 빼고 loss를 구하기 위해\n",
    "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def accuracy(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
    "    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\n",
    "    pred *= mask\n",
    "    acc = train_accuracy(real, pred)\n",
    "    \n",
    "    return tf.reduce_mean(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fa3774-28d1-41a4-9635-1f1fe344b72c",
   "metadata": {},
   "source": [
    "## Main Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f4c9465-f8d7-4481-9cbc-a0aceb506035",
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq(tf.keras.Model):\n",
    "    def __init__(self, enc_vocab_size, dec_vocab_size, embedding_dim, enc_dim, dec_dim, batch_size, end_token_idx=3):\n",
    "        super(seq2seq, self).__init__()\n",
    "        # 문장의 끝 토큰 [EOS] index - 3 \n",
    "        self.end_token_idx = end_token_idx\n",
    "        \n",
    "        self.encoder = Encoder(enc_vocab_size, embedding_dim, enc_dim, batch_size)\n",
    "        self.decoder = Decoder(dec_vocab_size, embedding_dim, dec_dim, batch_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        # encoder, decoder input\n",
    "        input_, target = x\n",
    "        \n",
    "        # encoder 초기값 설정\n",
    "        enc_hidden = self.encoder.init_hidden(input_)\n",
    "        # encoder의 RNN 연산 후 출력값\n",
    "        enc_out, enc_hidden = self.encoder(input_, enc_hidden)\n",
    "        \n",
    "        # dec_hidden 초기값 지정\n",
    "        dec_hidden = enc_hidden\n",
    "        \n",
    "        predict_tokens = list()\n",
    "        for t in range(target.shape[1]):\n",
    "            # decoder input에 시계열 축 추가 (None, 1, 1)\n",
    "            dec_input = tf.dtypes.cast(tf.expand_dims(target[:, t], 1), tf.float32)\n",
    "            # decoder RNN 연산 결과\n",
    "            predictions, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_out)\n",
    "            predict_tokens.append(tf.dtypes.cast(predictions, tf.float32))\n",
    "            \n",
    "        return tf.stack(predict_tokens, axis=1)\n",
    "    \n",
    "    def inference(self, x):\n",
    "        input_ = x\n",
    "        \n",
    "        enc_hidden = self.encoder.init_hidden(input_)\n",
    "        enc_out, enc_hidden = self.encoder(input_, enc_hidden)\n",
    "        \n",
    "        dec_hidden = enc_hidden\n",
    "        \n",
    "        dec_input = np.array([2]) # [BOF] index\n",
    "        dec_input = tf.expand_dims(dec_input, 1)\n",
    "        \n",
    "        predict_tokens = list()\n",
    "        for t in range(0, MAX_SEQUENCE):\n",
    "            predictions, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_out)\n",
    "            predict_token = tf.argmax(predictions[0])\n",
    "            \n",
    "            if predict_token == self.end_token_idx:\n",
    "                break\n",
    "                \n",
    "            predict_tokens.append(predict_token)\n",
    "            dec_input = tf.dtypes.cast(tf.expand_dims([predict_token], 0), tf.float32)\n",
    "        \n",
    "        return tf.stack(predict_tokens, axis=0).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a704ad2-33a3-429a-a4aa-9ded323716ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5408bcf3-f18c-4b98-ba9c-d3d6941ab42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190200</th>\n",
       "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
       "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190201</th>\n",
       "      <td>Death is something that we're often discourage...</td>\n",
       "      <td>La mort est une chose qu'on nous décourage sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190202</th>\n",
       "      <td>Since there are usually multiple websites on a...</td>\n",
       "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190203</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190204</th>\n",
       "      <td>It may be impossible to get a completely error...</td>\n",
       "      <td>Il est peut-être impossible d'obtenir un Corpu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190205 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      eng  \\\n",
       "0                                                     Go.   \n",
       "1                                                     Go.   \n",
       "2                                                     Hi.   \n",
       "3                                                     Hi.   \n",
       "4                                                    Run!   \n",
       "...                                                   ...   \n",
       "190200  A carbon footprint is the amount of carbon dio...   \n",
       "190201  Death is something that we're often discourage...   \n",
       "190202  Since there are usually multiple websites on a...   \n",
       "190203  If someone who doesn't know your background sa...   \n",
       "190204  It may be impossible to get a completely error...   \n",
       "\n",
       "                                                      fra  \n",
       "0                                                 Marche.  \n",
       "1                                                 Bouge !  \n",
       "2                                                 Salut !  \n",
       "3                                                  Salut.  \n",
       "4                                                 Cours !  \n",
       "...                                                   ...  \n",
       "190200  Une empreinte carbone est la somme de pollutio...  \n",
       "190201  La mort est une chose qu'on nous décourage sou...  \n",
       "190202  Puisqu'il y a de multiples sites web sur chaqu...  \n",
       "190203  Si quelqu'un qui ne connaît pas vos antécédent...  \n",
       "190204  Il est peut-être impossible d'obtenir un Corpu...  \n",
       "\n",
       "[190205 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/fra.txt', sep='\\t')\n",
    "data.columns=['eng', 'fra', 'etc']\n",
    "data.drop('etc', axis=1, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5507dbac-3614-4624-a70f-7a288408ccdb",
   "metadata": {},
   "source": [
    "## 전처리\n",
    "* 띄어쓰기 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba7a6a9b-2c0c-434a-89f8-1bf53ca7cf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(x):\n",
    "    text_eng = x['eng']\n",
    "    text_fra = x['fra']\n",
    "    \n",
    "    text_eng = re.sub(r\"([!.,?])\", r\" \\1\", text_eng)\n",
    "    text_fra = re.sub(r\"([!.,?])\", r\" \\1\", text_fra)\n",
    "    \n",
    "    text_eng = re.sub(r\"[^a-zA-Z?.,!]+\", \" \", text_eng)\n",
    "    text_fra = re.sub(r\"[^a-zA-Z?.,!]+\", \" \", text_fra)\n",
    "    \n",
    "    return text_eng, text_fra\n",
    "\n",
    "def sentence2length(x):\n",
    "    x['preprocessed_eng'].split()\n",
    "    x['preprocessed_fra'].split()\n",
    "    \n",
    "    return eng, fra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37822cf3-a982-414a-82c6-5a2611a38f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>preprocessed_eng</th>\n",
       "      <th>preprocessed_fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>190200</th>\n",
       "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
       "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
       "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
       "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190201</th>\n",
       "      <td>Death is something that we're often discourage...</td>\n",
       "      <td>La mort est une chose qu'on nous décourage sou...</td>\n",
       "      <td>Death is something that we re often discourage...</td>\n",
       "      <td>La mort est une chose qu on nous d courage sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190202</th>\n",
       "      <td>Since there are usually multiple websites on a...</td>\n",
       "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
       "      <td>Since there are usually multiple websites on a...</td>\n",
       "      <td>Puisqu il y a de multiples sites web sur chaqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190203</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n",
       "      <td>If someone who doesn t know your background sa...</td>\n",
       "      <td>Si quelqu un qui ne conna t pas vos ant c dent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190204</th>\n",
       "      <td>It may be impossible to get a completely error...</td>\n",
       "      <td>Il est peut-être impossible d'obtenir un Corpu...</td>\n",
       "      <td>It may be impossible to get a completely error...</td>\n",
       "      <td>Il est peut tre impossible d obtenir un Corpus...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      eng  \\\n",
       "190200  A carbon footprint is the amount of carbon dio...   \n",
       "190201  Death is something that we're often discourage...   \n",
       "190202  Since there are usually multiple websites on a...   \n",
       "190203  If someone who doesn't know your background sa...   \n",
       "190204  It may be impossible to get a completely error...   \n",
       "\n",
       "                                                      fra  \\\n",
       "190200  Une empreinte carbone est la somme de pollutio...   \n",
       "190201  La mort est une chose qu'on nous décourage sou...   \n",
       "190202  Puisqu'il y a de multiples sites web sur chaqu...   \n",
       "190203  Si quelqu'un qui ne connaît pas vos antécédent...   \n",
       "190204  Il est peut-être impossible d'obtenir un Corpu...   \n",
       "\n",
       "                                         preprocessed_eng  \\\n",
       "190200  A carbon footprint is the amount of carbon dio...   \n",
       "190201  Death is something that we re often discourage...   \n",
       "190202  Since there are usually multiple websites on a...   \n",
       "190203  If someone who doesn t know your background sa...   \n",
       "190204  It may be impossible to get a completely error...   \n",
       "\n",
       "                                         preprocessed_fra  \n",
       "190200  Une empreinte carbone est la somme de pollutio...  \n",
       "190201  La mort est une chose qu on nous d courage sou...  \n",
       "190202  Puisqu il y a de multiples sites web sur chaqu...  \n",
       "190203  Si quelqu un qui ne conna t pas vos ant c dent...  \n",
       "190204  Il est peut tre impossible d obtenir un Corpus...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['preprocessed_eng'] = data.apply(lambda x: preprocessing(x)[0], axis=1)\n",
    "data['preprocessed_fra'] = data.apply(lambda x: preprocessing(x)[1], axis=1)\n",
    "data.tail()\n",
    "# seq_length = data.apply(lambda x: data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "391a998a-46fb-4db5-8b1c-46211e56a21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 66)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가장 긴 문장은?\n",
    "engLen = data['preprocessed_eng'].apply(lambda x: len(x.split()))\n",
    "fraLen = data['preprocessed_fra'].apply(lambda x: len(x.split()))\n",
    "engLen.max(), fraLen.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d8071-5548-4147-8b9f-3b5b34211d29",
   "metadata": {},
   "source": [
    "## 영어, 프랑스어 단어 사전 만들기(word->id, id->word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fd1069a-4d0c-4ece-b781-992d10ef4d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_word_to_id = {'[PAD]': 0, '[UNK]': 1, '[BOS]': 2, '[EOS]': 3}\n",
    "fra_word_to_id = {'[PAD]': 0, '[UNK]': 1, '[BOS]': 2, '[EOS]': 3}\n",
    "\n",
    "words_eng = \"\"\n",
    "for eng in data['preprocessed_eng']:\n",
    "    words_eng += eng + \" \"\n",
    "words_eng = list(set(words_eng.split()))\n",
    "\n",
    "for word_eng in words_eng:\n",
    "    eng_word_to_id[word_eng] = len(eng_word_to_id)\n",
    "\n",
    "words_fra = \"\"\n",
    "for fra in data['preprocessed_fra']:\n",
    "    words_fra += fra + \" \"\n",
    "words_fra = list(set(words_fra.split()))\n",
    "\n",
    "for word_fra in words_fra:\n",
    "    fra_word_to_id[word_fra] = len(fra_word_to_id)\n",
    "\n",
    "# len(vocab_eng), len(vocab_fra) # (17920, 33067)\n",
    "# len(eng_word_to_id), len(fra_word_to_id)\n",
    "\n",
    "# 각 숫자별 단어 부여\n",
    "id_to_eng_word = {_id:word for word, _id in eng_word_to_id.items()}\n",
    "id_to_fra_word = {_id:word for word, _id in fra_word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d958155d-bafc-431f-9a42-67cdd2e3f96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "labels = []\n",
    "MAX_SEQUENCE = 70\n",
    "shffled_data = data.sample(frac=1).copy()\n",
    "shffled_data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "inputs_eng = []\n",
    "for eng_sentence in shffled_data['preprocessed_eng']:\n",
    "    row = [eng_word_to_id[eng_word] for eng_word in eng_sentence.split()]\n",
    "    row += [0] * (MAX_SEQUENCE - len(row))\n",
    "    inputs_eng.append(row)\n",
    "\n",
    "inputs_fra = []\n",
    "for fra_sentence in shffled_data['preprocessed_fra']:\n",
    "    # decoder 입력과 label 생성\n",
    "    row = [fra_word_to_id['[BOS]']]\n",
    "    row_label = [fra_word_to_id[fra_word] for fra_word in fra_sentence.split()]\n",
    "    row += row_label\n",
    "    row_label += [fra_word_to_id['[EOS]']]\n",
    "    row += [0] * (MAX_SEQUENCE - len(row))\n",
    "    row_label += [0] * (MAX_SEQUENCE - len(row_label))\n",
    "    inputs_fra.append(row)\n",
    "    labels.append(row_label)\n",
    "    \n",
    "inputs.append(inputs_eng)\n",
    "inputs.append(inputs_fra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09af4b4e-fe82-42bf-ad07-15e4e90c9d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 190205, 70), (190205, 70))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = np.array(inputs)\n",
    "labels = np.array(labels)\n",
    "inputs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41047929-ebcf-44a7-8f64-93fcb203e2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 114123, 70), (2, 38041, 70), (2, 38041, 70))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx = int(len(labels) * 0.6)\n",
    "val_idx = (len(labels) - train_idx)//2 + train_idx\n",
    "\n",
    "train_X = inputs[:,:train_idx,:]\n",
    "val_X = inputs[:,train_idx:val_idx,:]\n",
    "test_X = inputs[:,val_idx:,:]\n",
    "\n",
    "train_y = labels[:train_idx,:]\n",
    "val_y = labels[train_idx:val_idx,:]\n",
    "test_y = labels[val_idx:,:]\n",
    "\n",
    "train_X.shape, val_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dfb1d8-daa7-412d-bdd2-ee58ebe2d02e",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c07f34a9-aabd-4ee7-9ad4-6204b9dfdcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dim = 256\n",
    "dec_dim = 256  # 256 # bidirection RNN이 아닐 경우\n",
    "embedding_dim = 256\n",
    "batch_size = 32\n",
    "EPOCH = 50\n",
    "\n",
    "model = seq2seq(len(eng_word_to_id),\n",
    "                len(fra_word_to_id),\n",
    "                embedding_dim,\n",
    "                enc_dim,\n",
    "                dec_dim,\n",
    "                batch_size)\n",
    "\n",
    "model.compile(loss=loss,\n",
    "             optimizer=tf.keras.optimizers.Adam(),\n",
    "             metrics=[accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c217735-56b6-4e1a-84c1-b7150781c601",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method seq2seq.call of <__main__.seq2seq object at 0x000002B9732CD0D0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method seq2seq.call of <__main__.seq2seq object at 0x000002B9732CD0D0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Decoder.call of <__main__.Decoder object at 0x000002B97330C910>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Decoder.call of <__main__.Decoder object at 0x000002B97330C910>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "3567/3567 [==============================] - ETA: 0s - loss: 0.5537 - accuracy: 0.8976\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.91760, saving model to model\\weights.h5\n",
      "3567/3567 [==============================] - 2023s 567ms/step - loss: 0.5537 - accuracy: 0.8976 - val_loss: 0.3488 - val_accuracy: 0.9176\n",
      "Epoch 2/5\n",
      "2239/3567 [=================>............] - ETA: 11:39 - loss: 0.2735 - accuracy: 0.9248"
     ]
    }
   ],
   "source": [
    "import os\n",
    "MODEL_PATH = \"model\"\n",
    "if not(os.path.isdir(MODEL_PATH)):\n",
    "    os.makedirs(os.path.join(MODEL_PATH))\n",
    "    \n",
    "checkpoint_path = MODEL_PATH + '/weights.h5'\n",
    "\n",
    "cp_callback = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', verbose=1,\n",
    "                              save_best_only=True, save_weights_only=True)\n",
    "\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=16)\n",
    "\n",
    "history = model.fit([train_X[0], train_X[1]], train_y,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=EPOCH,\n",
    "                    validation_data=((val_X[0], val_X[1]), val_y),\n",
    "                    callbacks=[earlystop_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8220c0d5-b895-4ff3-a60f-6f9611a57463",
   "metadata": {},
   "source": [
    "# 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "835e1642-a729-4402-bc65-c2477e0f19f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70,) (70,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 70)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_enc, x_dec = np.zeros(MAX_SEQUENCE), np.zeros(MAX_SEQUENCE)\n",
    "print(x_enc.shape, x_dec.shape)\n",
    "x_enc = x_enc.reshape(-1,x_enc.shape[0])\n",
    "x_dec = x_dec.reshape(-1,x_dec.shape[0])\n",
    "x_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31048f36-73e9-4226-916a-7ef907d86290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70,) (70,)\n",
      "(1, 70) (1, 70)\n"
     ]
    }
   ],
   "source": [
    "t1 = test_X[0][-100]\n",
    "t2 = test_X[1][-100]\n",
    "print(t1.shape, t2.shape)\n",
    "t1 = t1.reshape(-1,t1.shape[0])\n",
    "t2 = t2.reshape(-1,t2.shape[0])\n",
    "print(t1.shape, t2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03786e13-38f8-4df6-b3cb-61dca785a7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer seq2seq is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model([x_enc, x_dec])\n",
    "\n",
    "model.load_weights('model/weights-17.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "862d83ea-100f-4603-b0f4-6fe960e7b97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"seq2seq\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Encoder)            multiple                  4667136   \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            multiple                  12352398  \n",
      "=================================================================\n",
      "Total params: 17,019,534\n",
      "Trainable params: 17,019,534\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23561b51-e091-499a-9df9-411c8f4785de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She can jump high .\n",
      "[ 2659  9328 13094 21752  1063 18291  2002   197  7193 14414 12607  3328\n",
      " 10009 15016 12585  6548 14404]\n",
      "fruitiers console accueillante puissantes remplit laissent craintif couchais historique rayon estivales disco Et vidences papillons maisons suivit\n"
     ]
    }
   ],
   "source": [
    "sentence = test_X[0][38038]\n",
    "print(' '.join([id_to_eng_word[token] for token in sentence if token != 0]))\n",
    "sentence = sentence.reshape(-1, sentence.shape[0])\n",
    "pred = model.inference(sentence)\n",
    "print(pred)\n",
    "print(' '.join([id_to_fra_word[token] for token in pred if token != 0]))\n",
    "# pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b87c2d6d-d6ef-480d-b43e-17dace2df1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BOS] Elle peut sauter haut .\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([id_to_fra_word[token] for token in test_X[1][38038] if token != 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5e8c7c9-dac5-42c9-ac14-a86ee53f4605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16424</td>\n",
       "      <td>10644</td>\n",
       "      <td>790</td>\n",
       "      <td>12904</td>\n",
       "      <td>5141</td>\n",
       "      <td>4303</td>\n",
       "      <td>14552</td>\n",
       "      <td>7842</td>\n",
       "      <td>1458</td>\n",
       "      <td>907</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2876</td>\n",
       "      <td>3535</td>\n",
       "      <td>12833</td>\n",
       "      <td>7923</td>\n",
       "      <td>11813</td>\n",
       "      <td>5136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2372</td>\n",
       "      <td>16166</td>\n",
       "      <td>12030</td>\n",
       "      <td>12795</td>\n",
       "      <td>790</td>\n",
       "      <td>12272</td>\n",
       "      <td>5136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14251</td>\n",
       "      <td>5991</td>\n",
       "      <td>1954</td>\n",
       "      <td>4303</td>\n",
       "      <td>16029</td>\n",
       "      <td>2190</td>\n",
       "      <td>2350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13349</td>\n",
       "      <td>10651</td>\n",
       "      <td>12833</td>\n",
       "      <td>16588</td>\n",
       "      <td>10372</td>\n",
       "      <td>5136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38036</th>\n",
       "      <td>4303</td>\n",
       "      <td>880</td>\n",
       "      <td>2941</td>\n",
       "      <td>11222</td>\n",
       "      <td>13120</td>\n",
       "      <td>5016</td>\n",
       "      <td>7837</td>\n",
       "      <td>9933</td>\n",
       "      <td>5136</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38037</th>\n",
       "      <td>9373</td>\n",
       "      <td>14038</td>\n",
       "      <td>11739</td>\n",
       "      <td>5991</td>\n",
       "      <td>12469</td>\n",
       "      <td>13120</td>\n",
       "      <td>2350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38038</th>\n",
       "      <td>15018</td>\n",
       "      <td>16166</td>\n",
       "      <td>13593</td>\n",
       "      <td>11355</td>\n",
       "      <td>5136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38039</th>\n",
       "      <td>4303</td>\n",
       "      <td>9503</td>\n",
       "      <td>5517</td>\n",
       "      <td>16436</td>\n",
       "      <td>5136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38040</th>\n",
       "      <td>91</td>\n",
       "      <td>7639</td>\n",
       "      <td>5136</td>\n",
       "      <td>9319</td>\n",
       "      <td>4658</td>\n",
       "      <td>15077</td>\n",
       "      <td>50</td>\n",
       "      <td>16592</td>\n",
       "      <td>8701</td>\n",
       "      <td>10807</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38041 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0      1      2      3      4      5      6      7     8      9   \\\n",
       "0      16424  10644    790  12904   5141   4303  14552   7842  1458    907   \n",
       "1       2876   3535  12833   7923  11813   5136      0      0     0      0   \n",
       "2       2372  16166  12030  12795    790  12272   5136      0     0      0   \n",
       "3      14251   5991   1954   4303  16029   2190   2350      0     0      0   \n",
       "4      13349  10651  12833  16588  10372   5136      0      0     0      0   \n",
       "...      ...    ...    ...    ...    ...    ...    ...    ...   ...    ...   \n",
       "38036   4303    880   2941  11222  13120   5016   7837   9933  5136      0   \n",
       "38037   9373  14038  11739   5991  12469  13120   2350      0     0      0   \n",
       "38038  15018  16166  13593  11355   5136      0      0      0     0      0   \n",
       "38039   4303   9503   5517  16436   5136      0      0      0     0      0   \n",
       "38040     91   7639   5136   9319   4658  15077     50  16592  8701  10807   \n",
       "\n",
       "       ...  60  61  62  63  64  65  66  67  68  69  \n",
       "0      ...   0   0   0   0   0   0   0   0   0   0  \n",
       "1      ...   0   0   0   0   0   0   0   0   0   0  \n",
       "2      ...   0   0   0   0   0   0   0   0   0   0  \n",
       "3      ...   0   0   0   0   0   0   0   0   0   0  \n",
       "4      ...   0   0   0   0   0   0   0   0   0   0  \n",
       "...    ...  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  \n",
       "38036  ...   0   0   0   0   0   0   0   0   0   0  \n",
       "38037  ...   0   0   0   0   0   0   0   0   0   0  \n",
       "38038  ...   0   0   0   0   0   0   0   0   0   0  \n",
       "38039  ...   0   0   0   0   0   0   0   0   0   0  \n",
       "38040  ...   0   0   0   0   0   0   0   0   0   0  \n",
       "\n",
       "[38041 rows x 70 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = pd.DataFrame(test_X[0])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a35274-5ba6-4395-abf7-0514fbb10afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_Project",
   "language": "python",
   "name": "nlp_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
